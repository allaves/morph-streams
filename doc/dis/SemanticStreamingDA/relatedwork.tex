\section{Background}
\label{previousworks}

%\ \\
%- what has been done till now in the subjects:
%   streaming data + SNs
%   obda
%   obda integration
%   streaming rdf queries
%\ \\

The following sections describe the state of the art in streaming data access and continuous queries (Section \ref{datastreams}), ontology-based data access (Section \ref{obda}), and query languages for RDF streams (\ref{rdfstreams}).% We focus on the works and technologies that we used, extended and integrated for our approach: R\subscript{2}O, ODEMapster and SNEEql.

%The following sections describe the state of the art in the areas we base our work on. First in Section \ref{datastreams} we introduce the fundamentals of streaming data access and continuous queries. Then in Section \ref{obda} we provide a short overview of the research and available solutions for ontology-based data access.
%Next in Section \ref{obdi} we present the challenges of data integration and the relevant works on the subject that consider the problem of semantic interoperability and mappings with relational sources.
%In Section \ref{rdfstreams} we review other works dealing with semantic queries over streams and the specific extensions to currently available query languages.


\subsection{Streaming Data Access}
\label{datastreams}

%\ \\
%- streaming data basics, what is stream, windows, cont queries
%- special case event streams and SNs
%- SNEEql
%- some semantics
%- existing systems
%\ \\

Streaming data is characterised by the fact that it is normally transient and potentially infinite, with new data items
being regularly added and where old items are usually less relevant than newer ones. Hence Data Stream Management
Systems (DSMS) are quite different from classical database systems, which deal mostly with static data, with lower
insert rates and queries that retrieve the state of the data at the current time. Stream systems require additional
operators in their query languages, such as time-based windows to limit streams to finite bounded structures in order
to process only a smaller subset of data \cite{Arasu_2006,Brenninkmeijer_08}.
%Windows are widely used to perform such transformations from streams to bounded extents such as relations \cite{Arasu_2006,Brenninkmeijer_08}. A time window takes a time interval as parameter and returns a relation containing tuples of the stream whose timestamp falls in that specified time interval \cite{Babcock_02}. It can be said that a time window returns a finite snapshot of the stream in a given interval. %Otherwise it would be difficult to perform operations such as aggregates, as streaming data is unbounded.
%Although continuous queries over data streams operate over the newest items, it is also useful to take into account older information. Several approaches exist to deal with this issue. For example, significant samples of past received data can be stored in static tables or aggregates (e.g. average) can also be stored in some repository [BBD+02]. % Storage and processing of all the archived data is also a possibility but it is impractical in many scenarios, due to the high volumes of collected data.

%Several Data Stream Management Systems (DSMS) or systems managing continuous data, have been designed and built in the past years, such as
%Tapestry \cite{Terry_92},
%Tribeca \cite{Sullivan_96},%out
%TelegraphCQ \cite{Chandrasekaran_03},
%Aurora \cite{Abadi_2003,Carney_02},
%STREAM \cite{Arasu_06a},
%Borealis \cite{Abadi_2005},
%NiagaraCQ \cite{Chen_00},%out
%Gigascope \cite{Cranor_03},%out
%CAPE \cite{Rundensteiner_04}, %out
%TinyDB \cite{Madden_05},
%Cougar \cite{Yao_02} and
%SNEE \cite{Galpin_09}, among others.

Several DSMS have been built in the past years and can be grouped in two main areas: event stream systems (e.g. Aurora/Borealis \cite{Abadi_2005}, STREAM \cite{Arasu_06a}, TelegraphCQ \cite{Chandrasekaran_03}) and acquisitional stream systems (e.g. TinyDB \cite{Madden_05}, SNEE \cite{Galpin_09}, Cougar \cite{Yao_02}). For the first, the stream system does not have control over the data arrival rate, which is often potentially high and usually unknown. For acquisitional streams, it is possible to control when data is obtained from the source.

All these systems have their own continuous query language, generally based on SQL, although most of them share the
same features.
\ %Several restrictions must be considered in this case, namely the usually low energy resources, limited computing power and storage capabilities of sensors. In order to address these issues, research has produced Sensor Networks Query Processing engines.% such as TinyDB, Cougar and SNEE.
%These processors use declarative query languages for continuous data which describe logically the set of information that is to be collected but leaves to the engine to determine the algorithms and plans that are needed to get the data form the different nodes of the sensor network. Hence the server engine produces optimised query plans that are locally executed by the sensor network nodes in a distributed in-network scheme. These engines must also consider several optimisation techniques in order to efficiently gather the information from the sensor nodes. This approach has been proven to be efficient especially in terms of energy consumption \cite{Madden_05}. Architectures for query optimisation in these constrained scenarios have surfaced \cite{Galpin_09,Madden_05}, showing that even with such limitations it is still possible to use rich and expressive declarative query languages.
%Some of these systems have their own stream query language although all are based somehow on SQL. Although CQL (Continuous Query Language) \cite{Arasu_2006} has been regarded as the most prominent and best known of these languages  there is still no common language for stream queries.
In order to exemplify these query language features we will use the syntax of one of them: SNEEql
\cite{Brenninkmeijer_08} (which is based on CQL).
%
% and although some standardisation attempts have surfaced \cite{Jain_08},
%The SNEEql [BGF+08] language for querying streaming data sources is based on the widely known CQL [ABW06], but it provides greater expressiveness in queries, including streams and relations, time and tuple windows, blocking and non-blocking operators, and pull and pushed based streams.
%
%In streaming data models the basic compound tuple type is defined as a set of typed attributes. A tagged tuple in a stream is a tuple that includes a named timestamp attribute.

The first concept to be considered in stream data models is that of a \textit{tagged tuple}, which is a tuple that
includes a named \textit{timestamp} attribute. This special attribute indicates when the tuple entered the stream and
is essential to define the semantics of stream operators in these languages: two tuples having the same timestamp are
considered to have entered the stream at the same time instant. A stream is a potentially infinite sequence of tagged
tuples.

Next we can move into \emph{queries}. Queries over streams are of the form:
\begin{align*}
\mathsf{SELECT}\ \langle\mathsf{*STREAM}\rangle\ a_1,\ldots ,a_n\ \mathsf{FROM}\ w_1 \langle \mathsf{window} \rangle,\ldots,w_m \langle \mathsf{window} \rangle\ \mathsf{WHERE}\ p
\end{align*}
where $a1, \ldots ,a_n$ is a project list, $w_1, \ldots , w_m$ is a list of streams of tagged tuples with optional
window definitions, and $p$ is a predicate \cite{Arasu_2006,Brenninkmeijer_08}. The result of the execution of a stream
query is a stream of tagged tuples or a stream of windows.
%Stream systems require operators to limit streams to finite bounded structures in order to process only a smaller subset of data.
%Consider for example that we want to obtain the latest temperature values measured at the sea level. This is a subset of data that must be obtained from a stream of continuously appended temperature measurements.
%Windows are widely used to perform such transformations from streams to a bounded extents such relations [CQL,BGF+08]. The most common stream-to-relation operators are time windows.
%A time window operator produces bounded sequences of tagged tuples whose timestamp falls in the specified interval. Window queries are of the form:
%\begin{align*}
%$\mathsf{SELECT}\\ a_1,\ldots,a_n\ \mathsf{FROM}\ w_1,\ldots,w_m\ \mathsf{WHERE}\ p$
%\end{align*}
%, where $a_1, \ldots ,a_n$ is a projection list of attributes of the stream, $w_1, \ldots , w_m$ is a list of window definitions, and $p$ is a predicate \cite{Arasu_2006,Brenninkmeijer_08}.
%The result of the execution of a window query is a stream of windows.

In queries, a \emph{time window} operator produces bounded sequences of tagged tuples whose timestamp falls in the
specified interval. A window can be specified as follows: $s$ \textsf{[FROM} $t_1$ \textsf{TO} $t_2$ \textsf{SLIDE int
unit]} where \textsf{FROM} $t_1$ \textsf{TO} $t_2$ indicates a time interval. The slide parameter indicates the
frequency of the window creation in time units or rows.
%A window on a table can be specified as follows: $t$
%\textsf{[SCAN int timeUnit]}
%where $t$ is a table. The scan parameter indicates the frequency of the table scan and its corresponding windows creation \cite{Brenninkmeijer_08}.
Notice that windows are not only time dependant, but may also be tuple (row) dependant.

Other important and useful features of continuous query languages are \emph{aggregation functions},
\emph{window-to-stream operators} such as \textsf{ISTREAM, DSTREAM} and \textsf{RSTREAM} \cite{Arasu_2006}, and
\emph{quality of service requirements} \cite{Galpin_09} (acquisition rate, delivery time, network lifetime, etc.).

%Window-to-stream operators are also required for stream query languages, as the \textsf{ISTREAM, DSTREAM} and \textsf{RSTREAM} operators of CQL \cite{Arasu_2006}. The \textsf{ISTREAM} operator appends tuples that were not on the previous window, to the output stream. \textsf{DSTREAM} appends tuples that were deleted form the previous window. \textsf{RSTREAM} appends all tuples from the previous window to the output stream.
%Queries over streams usually require aggregation functions. Querying for sums, averages, maximums and minimums is quite common. Streaming query languages such as CQL and SNEEql provide support for this kind of operators, for either streams or relations.

%Another feature of some query languages is the possibility of specifying quality of service requirements. SNEEql allows for instance specifying the desired acquisition rate and delivery time:
%\textsf{ACQUISITION RATE = 3s ; DELIVERY TIME = 5s}
%These parameters can be used by the query engine to optimise the query plans and these can be extremely useful in the context of sensor networks because of the limited processing and power resources.

\lstdefinestyle{HaskellSNEE}{basicstyle=\ttfamily\scriptsize,
                        %keywordstyle=\lstuppercase,
                        emphstyle=\itshape,
                        showstringspaces=false,
                        }


\subsection{Ontology-based Data Access}
\label{obda}

%- idea behind, justify
%- previous systems
%- R2O
The goal of Ontology-based Data Access (OBDA) is to generate semantic web content from existing relational data sources available in the web \cite{Sahoo_09}.\
%The realisation of the Semantic Web vision, where data is available, understandable and processable by computers, has launched several initiatives that aim at providing semantic access to traditional data sources.
%Most stored data in the web is currently preserved in relational databases and it has therefore become a need to generate Semantic Web content from them \cite{Sahoo_09}. In this context there is a considerably large amount of research in the community, with the goal of exposing data in terms of ontologies that formally express a domain of interest \cite{Poggi_08}. This is the goal of Ontology-based data access (OBDA).
%Most of the approaches attempt to provide some kind of mapping from a relational concept to a concept in an ontological model.
%
%Many problems arise when dealing with model mismatch issues, query interpretation, semantic reasoning, etc.
As aforementioned, most of the existing approaches are based on the exploitation of mappings between the relational
(rows and columns) and the ontological (concepts and roles) models. Some of them use their own languages to define
these mappings, while others use SPARQL extensions or SQL expressions.
%OBDA systems use these kinds of mappings between an ontology and the different data sources.
In all cases, the objective of these systems is to allow constructing ontology-based queries (e.g. in SPARQL), which
are then rewritten into a set of queries expressed in the query language of the data source (typically SQL), according
to the specified mappings. The query results are then converted back from the relational format into RDF, which is
returned to the user.

%One common approach is to first generate a syntactical translation of the database schema to an ontological representation. Although the resulting ontology has no real semantics, it may be argued that this is a first step through an ontology model and that ontology alignment techniques could be used later to map it to a real domain ontology \cite{Lubyte_09}. Variations of ontology generation and syntactic mappings have been presented in previous works \cite{Sahoo_09}.%\cite{Seaborne_07,Cerbah_08,Laborda_06,Prudhommeaux_07}.
%In SquirrelRDF [SSW07] they take this simple approach. A rough mapping of the relational database schema is built in RDF. There is no mapping to a mediated ontology. SPARQL queries can be executed and results are return in RDF.
%An additional step is taken in RDBToOnto [Cer08], where the ontology generation process does not only take the database schema into account, but also the data. For instance it is able to discover subsumption relationships by finding categorisation columns in the database tables. Even after this resulting ontology is produced, RDBToOnto allows users to create custom constraints. However this work focuses on the ontology generation and does not provide a querying mechanism to the database data.
%Relational.OWL [PC06] builds an ontology based on the relational schema and then maps it to the mediated ontology through a RDF query language. The first phase -transforming the database schema to the Relational.OWL ontology- produces a syntactical representation without real semantics. It is thus necessary to proceed with the second step, mapping the Relational.OWL representation to the domain ontology. This mapping can be done with an RDF query language like SPARQL and it is therefore not necessary to create a new mapping language.
%The SPASQL implementation [Pru07] is an extension for MySQL to support SPARQL queries. It is thus technologically restricted to MySQL as it uses its query engine, although similar extensions could be built for other DBMSs. This extension is able to parse SPARQL queries and compile them and directly execute them in the MySQL engine. The mapping is limited (e.g. no multi-field keys allowed) and it is not formalized.

%Although automatic generation of ontologies and mappings can be useful in simple scenarios, for complex ones it is a
%limited approach. User expert knowledge may become necessary for complex mapping definitions, but it is also necessary
%to provide well defined languages that express those mappings. A number of OBDA systems use these mapping languages to
%access the underlying relational data sources \cite{Sahoo_09}.

%The Virtuoso \cite{Erling_07} declarative meta-schema language allows mapping relational schemas to RDF ontologies and is based on Quad Map Patterns that define the transformations. In the D2RQ platform \cite{Bizer_07}, the D2RQ language is introduced and formally defined by an RDF schema and its engine is implemented as a Jena graph. It provides a Jena or Sesame API plug-in for querying and reasoning that rewrites the API calls to SQL queries which are executed in the database server, using the mappings. MASTRO \cite{Poggi_07} is a OBDA system that works with ontologies whose data is accessed through mappings in an external source, i.e. using an OBDA-enabled reasoner. It works over the DL-LiteA language, a fragment of OWL-DL. The mappings are specified through assertions that include SQL queries over the database. The expressiveness of the queries is limited to conjunctive queries (CQ).
%The Virtuoso [EM07] declarative meta-schema language allows mapping relational schemas to RDF ontologies. SPARQL queries posed to the system are mapped to relational databases. The meta-schema is based on Quad Map Patterns that define transformations from relational columns into triples that match a SPARQL pattern. Notice that Virtuoso includes extensions to SPARQL for aggregates (COUNT, MIN, MAX, AVG, SUM) and it also allows SPARQL sub-queries.
%In the D2RQ platform [BC07], a specific mapping language is introduced (D2RQ language) and formally defined by an RDF schema. The mapping defines relationships between an ontology and a relational database schema. The D2RQ Engine is implemented as a Jena graph. It provides a Jena or Sesame API plug-in for querying and reasoning with RDF. This plug-in rewrites thee API calls to SQL queries which are executed in the database server, using the mappings. Results are returned as RDF triples.
%In [PLC+08] they propose MASTRO, a DL reasoner for ontologies whose data is accessed through mappings in an external source, i.e. an ODBA-enabled reasoner. The reasoner works over the DL-LiteA language, a fragment of OWL-DL. The mappings are specified through assertions that include SQL queries over the database. The language in this work is well formalized and the query answering complexity is well explored. The expressivity of the queries is limited to conjunctive queries (CQ). An implementation of a plug-in for data access to relational databases through SPARQL queries has also been built using this approach [PRR08].
%The R2O language [BCG05] provides formal mappings from relational databases to %ontologies. The ODEMapster System uses this language to execute queries on the %ontology that are translated to SQL to get results from the relational source.

There are two main alternative approaches for defining these mappings \cite{Lenzerini_02}, \textit{Local-as-view (LaV)}
and \textit{Global-as-view (GaV)}, and a combination of both, \textit{GLAV}. In the LaV approach, each of the source
schemas is represented as a view in terms of the global schema.
%If our system $\mathcal{I}$ is represented as: $\mathcal{I = <G,S,M>}$, where $\mathcal{G}$ is the global schema, $\mathcal{S}$ is a source schema and $\mathcal{M}$ is the mapping between $\mathcal{G}$ and $\mathcal{S}$, then the mapping assertions in $\mathcal{M}$ will be a set of elements of the form: $s \leadsto q_{\mathcal{G}}$
%where $s$ is an element of $\mathcal{S}$ and $q_{\mathcal{G}}$ is a query over the global schema $\mathcal{G}$.
This approach is useful if the global schema is well established or if the set of sources or their schemas may constantly change.
%Notice that changes in the sources do not affect the global schema.
However, query processing in this approach is not obvious, as it is not explicitly stated in the mapping definition how
to obtain the data from the global view.
%Query rewriting techniques such as query answering using views can be used in this approach \cite{Halevy_01}.
In the GaV approach, the global schema elements are represented as views over the source schemas and it is explicitly defined how to query the sources. The advantage is that the processor can directly use this information to perform the query rewriting. The main disadvantage is that mapping definitions are affected in case of changes in the set of sources or in any of their schemas.  % to obtain the desired information in terms of the global schema. %Following the system representation $\mathcal{I = <G,S,M>}$, in the GAV approach the mapping assertions are elements of the form:
%$g \leadsto q_{\mathcal{S}}$
%Where $g$ is an element of the global schema $\mathcal{G}$ and is expressed as a view $q_{\mathcal{S}}$, a query on the source schema.
%The advantage is that the mapping itself already indicates how to query the sources to obtain the data, so the processor can directly use this information to perform the query rewriting. The main disadvantage is that mappings definitions are affected in case of changes or additions to the sources schemas.%, the global schema may suffer changes and this may affect other mapping definitions.

We will now describe in detail one of these ODBA approaches (\rtwoo\ and ODEMapster), which is the one that we will
extend in this paper.

\subsubsection{\rtwoo\ and ODEMapster}
\label{R2O}
\rtwoo(Relational-to-Ontology)\cite{Barrasa_04} is a GaV mapping definition language that defines relationships between a set of ontologies and relational schemas. The \rtwoo\ language is XML-based, independent of any specific DBMS and allows complex mapping expressions between ontology and relational elements, %. \rtwoo\ mapping assertions can be created either manually or with the help of a mapping tool. %The mapping definition could also be used to perform validation of the database integrity by checking it against the ontology restrictions and axioms.
%It is important to notice that R2O does not define mappings in both senses, i.e. it does not provide a mapping definition from the ontology to the relational schema. The mapping assertions are designed to be used by applications and middleware systems, not by end users.
%The domain covered by the ontology and the database more than often differ. In some occasions they coincide on some concepts, so the domains intersect. In other occasions the ontology domain includes the relational database domain or vice versa.
%\rtwoo\ specifically considers classes, object and datatype properties in an ontology. They are
described in terms of selections and transformations over database tables and columns.
%In order to be able to describe these mappings, \rtwoo\ defines the ontology and database elements using its own XML notation.
\rtwoo\ covers a wide set of mapping cases common in relational to ontology situations. \rtwoo\ is designed to cope
with the following mapping cases:
\begin{itemize}
\item A database table maps to one class in the ontology. %Then the table columns map to attributes or relations of the concept. For each row in the table a corresponding instance in the ontology will be generated, with its attribute values filled with the columns data.
\item A single database table is mapped to more than one class in the ontology, and for each row a single instance of each class is generated. %In this case some columns will be mapped to a concept attributes while other columns will be mapped to other concept attributes.
\item A single database table is mapped to more than one class in the ontology, and multiple instances can be generated for each class. %It is a more general case than the previous one; multiple instances of the same ontology concept can be generated from a single database record.
\end{itemize}

Mapping tables and columns to concepts and attributes often requires performing some operations on the relational sources. Several cases are handled by \rtwoo\ and detailed below.
\begin{itemize}
\item \textit{Direct Mapping}. When the relational table maps an ontology class and the column values are used to fill the property values of the ontology instances. Each table record will generate a class instance in the ontology.
\item \textit{Join/Union}. In some occasions a single table does not correspond alone to a class, but it has to be combined with other tables. The result of the join or union of the tables will generate the corresponding ontology instances.
\item \textit{Projection}. Sometimes not all the columns are required for the mapping. The unnecessary columns can simply be ignored. In order to do so, a projection on the needed columns can be performed. % (e.g. a SELECT).
\item \textit{Selection}. In some situations not all the records of a table correspond to instances of the mapped ontology class. Then a subset of the records must be extracted. To do so, selection conditions can be applied to choose the desired subset for the mapping.
\end{itemize}
It is of course possible to combine joins, unions, projections and selections for more complex mapping definitions.
Values from the database can be copied as-is to the properties of instances in the ontology. However in many situations
it is necessary to perform some transformations on the values using some function. \rtwoo\ allows the use of defined
functions for this purpose, e.g. concatenation, sub-strings, arithmetic functions, etc.

The ODEMapster \cite{Barrasa_04} system is the processor that exploits \rtwoo\ mappings, offering a query language that
is a subset of SPARQL for conjunctive queries.

\subsection{Continuous Queries for RDF Streams}
\label{rdfstreams}
%- small motivation
%- C-sparql and Streaming sparql

%Debate limitations, all previous subjects are disconnected


%As it is described in Section \ref{datastreams} query languages for relational data streams have been proposed and implemented in recent years. These languages borrow much of relational query languages such as SQL. In ontology-based data access solutions, continuous queries are expected to be posed in terms of an ontological view. In order to do so, it is necessary to have a stream query language that natively supports semantic models \cite{DellaValle_09,Groppe_07}.\\

SPARQL \cite{Prudhommeaux_2008} is the W3C Recommendation for a query language over RDF. Even though  SPARQL has been used to query RDF triples annotated with time constructs \cite{Barbieri_2010,Bolles_08} and can be used to represent data coming from streaming sources, it currently lacks the necessary operators to effectively query streaming data.\
There are two main approaches in the literature for extending SPARQL with stream-based operators: Streaming SPARQL and C-SPARQL.
\subsubsection{Streaming SPARQL.}
In \cite{Bolles_08} extensions for SPARQL are provided, so that the resulting language is able to handle RDF based data streams. The semantics of these extensions are also provided as well as the algorithm to map the language additions to the extended algebra. The grammar of Streaming SPARQL basically consists in adding the capability of defining time and tuple-based windows over streams which are defined in the \textsf{FROM} clause. % which are explicitly stated using the \textsf{STREAM} keyword. The windows are defined in the \textsf{FROM} section, using the \textsf{WINDOW} keyword. Both time-based and tuple based windows are supported. For time-based windows the \textsf{RANGE} keyword allows specifying the window size in time units. A \textsf{SLIDE} parameter can be specified to indicate the frequency of the window creation.
%For tuple-base windows the ELEMS keyword is used instead of RANGE.
This proposal also allows specifying windows on graph patterns, which complicates its evaluation semantics. %It doesn't provide aggregate functions.
Here is an example of a Streaming SPARQL query, that obtains the sensor temperature values sensed in the latest 30 minutes, every minute:

\lstdefinelanguage{SPARQLSTR}[]{SQL}{
morekeywords={STREAM,PREFIX, WINDOW, RANGE, SLIDE, AGGREGATE, AVG, STEP, REGISTER , QUERY},
sensitive=true,%
morecomment=[l]\#,%
morestring=[b]',%
}



\lstdefinestyle{SPARQLSTRStyle}{basicstyle=\sffamily\scriptsize,
                        %keywordstyle=\lstuppercase,
                        emphstyle=\itshape,
                        showstringspaces=false,
                        tabsize=2,
                        }

\begin{lstlisting}[style=SPARQLSTRStyle,language=SPARQLSTR,frame=none]
PREFIX fire:<http://www.ssg4env.eu/fire#>
SELECT ?sensor ?temperature
FROM STREAM <www.ssg4env/Temperature.srdf>
WINDOW RANGE 30 MINUTE SLIDE 1 MINUTE
WHERE { ?sensor fire:hasTempMeasurement ?temperature .}
\end{lstlisting}

The operators correspond to a large extent to the operators seen in DSMS query languages. % like CQL (see Section \ref{datastreams}).
Instead of tagged tuples we have tagged triples and the time and tuple based attributes are similar as well in syntax
and semantics. However, Streaming SPARQL still lacks support for many features such as windows with higher boundaries
different to the current timestamp $now$, aggregates, projection functions, and acquisitional parameters, among others.
In addition, Streaming SPARQL allows windows in group graph patterns and redefines the language semantics due to the
introduction of timestamps.

\subsubsection{C-SPARQL.} C-SPARQL (Continuous SPARQL) \cite{Barbieri_2010} also works over RDF streams, sequences of triples annotated with non-decreasing timestamps. As in Streaming SPARQL, it defines both time or tuple-based sliding windows. C-SPARQL offers aggregates, such as \textsf{COUNT, SUM, AVG, MIN} and \textsf{MAX}. It also allows combining stored and streaming knowledge and also combining multiple streams. Here is an example of a C-SPARQL query, it obtains the temperature average of the values sensed in the last 10 minutes, every minute:

\begin{lstlisting}[style=SPARQLSTRStyle,language=SPARQLSTR,frame=none]
REGISTER QUERY AvergaeTemperature AS
PREFIX fire: <http://www.ssg4env.eu/fire#>
SELECT DISTINCT ?sensor ?average
FROM STREAM <www.ssg4env.eu/fire.srdf> [RANGE 10 MIN STEP 1 MIN]
WHERE { ?sensor fire:hasTempMeasurement ?temperature .}
AGGREGATE {(?average, AVG, {?temperature})}
\end{lstlisting}

Notice that neither language supports time windows with upper bounds different to $now$. Window-to-stream operators are
also missing in these specifications. Streaming SPARQL provides an extended algebra for these streaming features, but
it is the C-SPARQL approach the one that allows clearly separating the stream management and query evaluation concerns,
hence it will be the one that we will consider in our approach.
